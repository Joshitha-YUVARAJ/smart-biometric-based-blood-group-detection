{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS6vCNBOdDr_",
        "outputId": "90a6865c-41d9-40bf-b21c-6e75713c5e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rajumavinmar/finger-print-based-blood-group-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58.1M/58.1M [00:00<00:00, 184MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rajumavinmar/finger-print-based-blood-group-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQKyKSnBd2Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47877ac4"
      },
      "source": [
        "# Task\n",
        "Build a smart biometric system for instant blood group identification using fingerprint images. The system should achieve at least 95% accuracy. Save the trained model so it can be plugged into a full-stack application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "114b3074"
      },
      "source": [
        "## Data loading and exploration\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset and explore its structure, content, and characteristics. This includes understanding the file organization, image formats, and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Azur076eKc8",
        "outputId": "633e30a6-c3b3-4e7d-a039-bde42c2562e3"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group'\n",
        "\n",
        "# 1. Inspect the directory structure and count images per class\n",
        "class_counts = {}\n",
        "print(\"Dataset directory structure and image counts per class:\")\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    if root != dataset_path:\n",
        "        class_name = os.path.basename(root)\n",
        "        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        class_counts[class_name] = len(image_files)\n",
        "        print(f\"Directory: {root}, Class: {class_name}, Number of images: {len(image_files)}\")\n",
        "\n",
        "# 2. Print class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "# 3. Examine a few sample images from different classes\n",
        "print(\"\\nSample images from different classes:\")\n",
        "for class_name in class_counts.keys():\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    if image_files:\n",
        "        sample_image_name = random.choice(image_files)\n",
        "        sample_image_path = os.path.join(class_dir, sample_image_name)\n",
        "        try:\n",
        "            with Image.open(sample_image_path) as img:\n",
        "                print(f\"  Class: {class_name}, Sample Image: {sample_image_name}, Format: {img.format}, Size: {img.size}, Mode: {img.mode}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not open image {sample_image_name} in class {class_name}: {e}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory structure and image counts per class:\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/B+, Class: B+, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/O+, Class: O+, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/A+, Class: A+, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/B-, Class: B-, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/AB-, Class: AB-, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/A-, Class: A-, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/O-, Class: O-, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group/AB+, Class: AB+, Number of images: 0\n",
            "\n",
            "Class distribution:\n",
            "B+: 0\n",
            "O+: 0\n",
            "A+: 0\n",
            "B-: 0\n",
            "AB-: 0\n",
            "A-: 0\n",
            "O-: 0\n",
            "AB+: 0\n",
            "\n",
            "Sample images from different classes:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4aa236e",
        "outputId": "f4bdb21c-ac58-4fac-da49-08b0ff8c6ac3"
      },
      "source": [
        "import os\n",
        "\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group'\n",
        "\n",
        "print(f\"Listing contents of: {dataset_path}\")\n",
        "try:\n",
        "    for item in os.listdir(dataset_path):\n",
        "        item_path = os.path.join(dataset_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  Directory: {item}\")\n",
        "            # Optionally list contents of subdirectories if needed\n",
        "            # for sub_item in os.listdir(item_path):\n",
        "            #     print(f\"    - {sub_item}\")\n",
        "        else:\n",
        "            print(f\"  File: {item}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Directory not found at {dataset_path}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of: /root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group\n",
            "  Directory: B+\n",
            "  Directory: O+\n",
            "  Directory: A+\n",
            "  Directory: B-\n",
            "  Directory: AB-\n",
            "  Directory: A-\n",
            "  Directory: O-\n",
            "  Directory: AB+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3aa3545"
      },
      "source": [
        "**Reasoning**:\n",
        "The listing of the `dataset_blood_group` directory shows that the class directories (B+, O+, etc.) exist, but the previous attempt to count images within them resulted in zero counts and a FileNotFoundError when trying to sample images. This suggests the image files are not directly inside these top-level class directories. I need to modify the image counting and sampling logic to search for image files within these class directories and any potential subdirectories they might contain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1426b968",
        "outputId": "b543e265-ef16-43b2-d6dd-0a43ff4b497e"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group'\n",
        "\n",
        "# 1. Inspect the directory structure and count images per class by walking through subdirectories\n",
        "class_counts = {}\n",
        "print(\"Dataset directory structure and image counts per class:\")\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    class_name = os.path.basename(root)\n",
        "    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Only count files in the actual class directories, not the root dataset_blood_group dir\n",
        "    if root != dataset_path:\n",
        "        if class_name not in class_counts:\n",
        "            class_counts[class_name] = 0\n",
        "        class_counts[class_name] += len(image_files)\n",
        "        if len(image_files) > 0:\n",
        "             print(f\"Directory: {root}, Class: {class_name}, Number of images: {len(image_files)}\")\n",
        "\n",
        "\n",
        "# 2. Print class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "# 3. Examine a few sample images from different classes\n",
        "print(\"\\nSample images from different classes:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    if count > 0:\n",
        "        # Find all image files within the class directory and its subdirectories\n",
        "        class_images = []\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        for root, dirs, files in os.walk(class_dir):\n",
        "             for f in files:\n",
        "                 if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                     class_images.append(os.path.join(root, f))\n",
        "\n",
        "        if class_images:\n",
        "            sample_image_path = random.choice(class_images)\n",
        "            try:\n",
        "                with Image.open(sample_image_path) as img:\n",
        "                    print(f\"  Class: {class_name}, Sample Image: {os.path.basename(sample_image_path)}, Format: {img.format}, Size: {img.size}, Mode: {img.mode}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Could not open image {os.path.basename(sample_image_path)} in class {class_name}: {e}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory structure and image counts per class:\n",
            "\n",
            "Class distribution:\n",
            "B+: 0\n",
            "O+: 0\n",
            "A+: 0\n",
            "B-: 0\n",
            "AB-: 0\n",
            "A-: 0\n",
            "O-: 0\n",
            "AB+: 0\n",
            "\n",
            "Sample images from different classes:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2116f407",
        "outputId": "86adebd1-c9df-4436-d16a-d80cb9858407"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/rajumavinmar/finger-print-based-blood-group-dataset/versions/1/dataset_blood_group'\n",
        "\n",
        "# Dictionary to store image paths per class\n",
        "class_images = {}\n",
        "\n",
        "print(\"Searching for image files and counting per class...\")\n",
        "\n",
        "# Walk through all directories and files within the dataset_blood_group\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    # Determine the class name based on the directory path\n",
        "    # We assume the immediate subdirectory of dataset_blood_group is the class name\n",
        "    relative_path = os.path.relpath(root, dataset_path)\n",
        "    if relative_path != '.': # Exclude the root dataset_blood_group directory itself\n",
        "        class_name = relative_path.split(os.sep)[0]\n",
        "\n",
        "        if class_name not in class_images:\n",
        "            class_images[class_name] = []\n",
        "\n",
        "        for file in files:\n",
        "            # Check for common image extensions\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                class_images[class_name].append(file_path)\n",
        "\n",
        "\n",
        "# 1. Print image counts per class\n",
        "print(\"\\nImage counts per class:\")\n",
        "class_counts = {class_name: len(images) for class_name, images in class_images.items()}\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"Class: {class_name}, Number of images: {count}\")\n",
        "\n",
        "# 2. Print class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "\n",
        "# 3. Examine a few sample images from different classes\n",
        "print(\"\\nSample images from different classes:\")\n",
        "for class_name, images in class_images.items():\n",
        "    if images:\n",
        "        sample_image_path = random.choice(images)\n",
        "        try:\n",
        "            with Image.open(sample_image_path) as img:\n",
        "                print(f\"  Class: {class_name}, Sample Image: {os.path.basename(sample_image_path)}, Format: {img.format}, Size: {img.size}, Mode: {img.mode}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not open image {os.path.basename(sample_image_path)} in class {class_name}: {e}\")\n",
        "\n",
        "# 4. Note any potential issues (based on counts and sample inspection)\n",
        "print(\"\\nPotential issues noted during inspection:\")\n",
        "# Check for classes with zero images\n",
        "for class_name, count in class_counts.items():\n",
        "    if count == 0:\n",
        "        print(f\"- Class '{class_name}' has no images.\")\n",
        "\n",
        "# Note: Further issues like inconsistent sizes, formats, or quality would be observed during manual inspection of more samples or through automated analysis."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for image files and counting per class...\n",
            "\n",
            "Image counts per class:\n",
            "Class: B+, Number of images: 652\n",
            "Class: O+, Number of images: 852\n",
            "Class: A+, Number of images: 565\n",
            "Class: B-, Number of images: 741\n",
            "Class: AB-, Number of images: 761\n",
            "Class: A-, Number of images: 1009\n",
            "Class: O-, Number of images: 712\n",
            "Class: AB+, Number of images: 708\n",
            "\n",
            "Class distribution:\n",
            "B+: 652\n",
            "O+: 852\n",
            "A+: 565\n",
            "B-: 741\n",
            "AB-: 761\n",
            "A-: 1009\n",
            "O-: 712\n",
            "AB+: 708\n",
            "\n",
            "Sample images from different classes:\n",
            "  Class: B+, Sample Image: cluster_2_5746.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: O+, Sample Image: cluster_6_3536.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: A+, Sample Image: cluster_0_3044.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: B-, Sample Image: cluster_3_1886.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: AB-, Sample Image: cluster_5_4600.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: A-, Sample Image: cluster_1_3749.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: O-, Sample Image: cluster_7_3384.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "  Class: AB+, Sample Image: cluster_4_1695.BMP, Format: BMP, Size: (96, 103), Mode: RGBA\n",
            "\n",
            "Potential issues noted during inspection:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6d2abd8"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Preprocess the fingerprint images. This may involve resizing, normalization, and potentially data augmentation to increase the size and diversity of the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ba5190",
        "outputId": "23d4e999-89ce-4eb9-b9a7-2b67d6c6b3f4"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Define the target image size\n",
        "TARGET_SIZE = (128, 128)\n",
        "\n",
        "preprocessed_images = []\n",
        "labels = []\n",
        "label_map = {class_name: i for i, class_name in enumerate(class_images.keys())}\n",
        "\n",
        "# 2. Iterate through the class_images dictionary\n",
        "for class_name, image_paths in class_images.items():\n",
        "    print(f\"Preprocessing images for class: {class_name}\")\n",
        "    for image_path in image_paths:\n",
        "        try:\n",
        "            # a. Load each image using PIL\n",
        "            with Image.open(image_path) as img:\n",
        "                # b. Convert the image to RGB format\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                # c. Resize the image to the target size\n",
        "                img = img.resize(TARGET_SIZE)\n",
        "\n",
        "                # Convert image to numpy array\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                # d. Normalize the pixel values\n",
        "                img_array = img_array / 255.0\n",
        "\n",
        "                # e. Store the preprocessed image data and its corresponding class label\n",
        "                preprocessed_images.append(img_array)\n",
        "                labels.append(label_map[class_name])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process image {os.path.basename(image_path)}: {e}\")\n",
        "\n",
        "# 3. Convert lists to NumPy arrays\n",
        "preprocessed_images = np.array(preprocessed_images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"\\nShape of preprocessed_images: {preprocessed_images.shape}\")\n",
        "print(f\"Shape of labels: {labels.shape}\")\n",
        "\n",
        "# 4. Split the preprocessed data into training, validation, and test sets (Stratified Split)\n",
        "# First split: train and temp (validation + test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    preprocessed_images, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Second split: validation and test from temp\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nShape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing images for class: B+\n",
            "Preprocessing images for class: O+\n",
            "Preprocessing images for class: A+\n",
            "Preprocessing images for class: B-\n",
            "Preprocessing images for class: AB-\n",
            "Preprocessing images for class: A-\n",
            "Preprocessing images for class: O-\n",
            "Preprocessing images for class: AB+\n",
            "\n",
            "Shape of preprocessed_images: (6000, 128, 128, 3)\n",
            "Shape of labels: (6000,)\n",
            "\n",
            "Shape of X_train: (4800, 128, 128, 3), y_train: (4800,)\n",
            "Shape of X_val: (600, 128, 128, 3), y_val: (600,)\n",
            "Shape of X_test: (600, 128, 128, 3), y_test: (600,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f697e1"
      },
      "source": [
        "## Model selection and building\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable deep learning model architecture for image classification. Since fingerprint images are involved, a Convolutional Neural Network (CNN) based model would be a good starting point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T6zqlXotegdQ",
        "outputId": "898ed0b1-554b-4c58-a77e-4a4a392c78f9"
      },
      "source": [
        "%pip install tensorflow"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "bb7c06e2",
        "outputId": "5747bdb1-f882-4a7c-e280-fdf48aac94e0"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# 2. Define a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 3. Add convolutional layers\n",
        "# First convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)))\n",
        "# 4. Add pooling layers\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Third convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# 5. Flatten the output of the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# 6. Add dense (fully connected) layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# 7. Add the output layer\n",
        "num_classes = len(class_counts)\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 8. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,672\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,672</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,672\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,672</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27e79818"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the selected model on the preprocessed dataset. This involves splitting the data into training and validation sets, defining the loss function and optimizer, and training the model for a sufficient number of epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77407aed",
        "outputId": "a7389c4a-477b-4368-dba7-6faf631d4772"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. One-hot encode the training, validation, and testing labels\n",
        "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val_encoded = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=num_classes) # Although not used in training, it's good practice to encode it now.\n",
        "\n",
        "# 2. Train the compiled model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val_encoded)\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 356ms/step - accuracy: 0.4416 - loss: 1.4788 - val_accuracy: 0.8050 - val_loss: 0.4699\n",
            "Epoch 2/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 351ms/step - accuracy: 0.8316 - loss: 0.4364 - val_accuracy: 0.9000 - val_loss: 0.3035\n",
            "Epoch 3/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 350ms/step - accuracy: 0.8813 - loss: 0.3108 - val_accuracy: 0.8067 - val_loss: 0.5544\n",
            "Epoch 4/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 350ms/step - accuracy: 0.8875 - loss: 0.2950 - val_accuracy: 0.8933 - val_loss: 0.2784\n",
            "Epoch 5/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 348ms/step - accuracy: 0.9191 - loss: 0.2160 - val_accuracy: 0.9133 - val_loss: 0.2206\n",
            "Epoch 6/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 348ms/step - accuracy: 0.9172 - loss: 0.2192 - val_accuracy: 0.9067 - val_loss: 0.2519\n",
            "Epoch 7/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 350ms/step - accuracy: 0.9213 - loss: 0.1873 - val_accuracy: 0.9283 - val_loss: 0.1945\n",
            "Epoch 8/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 349ms/step - accuracy: 0.9380 - loss: 0.1606 - val_accuracy: 0.9000 - val_loss: 0.2457\n",
            "Epoch 9/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 346ms/step - accuracy: 0.9430 - loss: 0.1403 - val_accuracy: 0.9050 - val_loss: 0.2559\n",
            "Epoch 10/10\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 347ms/step - accuracy: 0.9525 - loss: 0.1228 - val_accuracy: 0.9133 - val_loss: 0.2502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efab16d6"
      },
      "source": [
        "## Model Optimization\n",
        "\n",
        "### Subtask:\n",
        "If the desired accuracy is not achieved, optimize the model by tuning hyperparameters, trying different model architectures, or incorporating advanced techniques like transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a65f5982",
        "outputId": "5c9bb9c0-ae1f-4dd6-f01b-d0743357690a"
      },
      "source": [
        "# 1. Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 2. (Optional) Further evaluation metrics: Precision, Recall, F1-score\n",
        "# To calculate these, we need to get the model's predictions\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2386\n",
            "Test Accuracy: 0.9233\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          B+       0.95      0.91      0.93        65\n",
            "          O+       0.97      0.86      0.91        86\n",
            "          A+       0.98      0.89      0.93        56\n",
            "          B-       0.93      1.00      0.96        74\n",
            "         AB-       0.95      0.92      0.93        76\n",
            "          A-       0.86      0.94      0.90       101\n",
            "          O-       0.84      0.90      0.87        71\n",
            "         AB+       0.96      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.92       600\n",
            "   macro avg       0.93      0.92      0.93       600\n",
            "weighted avg       0.93      0.92      0.92       600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49a3e963",
        "outputId": "8127017a-9e8d-41c1-e380-2e825747f9e1"
      },
      "source": [
        "# 1. Define the path and filename for saving the model\n",
        "model_save_path = \"blood_group_cnn_model.keras\"\n",
        "\n",
        "# 2. Save the trained model in the native Keras format\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved successfully to: {model_save_path}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully to: blood_group_cnn_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "1803dc2d",
        "outputId": "2b7c12b4-06d4-4ab1-8e77-e02d9c1635e3"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = \"blood_group_cnn_model.keras\"\n",
        "loaded_model = load_model(model_save_path)\n",
        "\n",
        "# Define the target image size (should be the same as used during training)\n",
        "TARGET_SIZE = (128, 128)\n",
        "\n",
        "# Define the label map (should be the same as used during training)\n",
        "# You might need to recreate this based on the order of classes in your training data\n",
        "# Assuming the order is the same as in the previous steps:\n",
        "label_map = {'B+': 0, 'O+': 1, 'A+': 2, 'B-': 3, 'AB-': 4, 'A-': 5, 'O-': 6, 'AB+': 7}\n",
        "# Create a reverse map for displaying results\n",
        "reverse_label_map = {i: class_name for class_name, i in label_map.items()}\n",
        "\n",
        "\n",
        "# Function to preprocess a single image from bytes\n",
        "def preprocess_single_image_from_bytes(image_bytes, target_size):\n",
        "    try:\n",
        "        img = Image.open(io.BytesIO(image_bytes))\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        img = img.resize(target_size)\n",
        "        img_array = np.array(img)\n",
        "        img_array = img_array / 255.0\n",
        "        # Add batch dimension\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process image from bytes: {e}\")\n",
        "        return None\n",
        "\n",
        "# Use files.upload() to get user input\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, file_bytes in uploaded.items():\n",
        "    print(f'User uploaded file \"{filename}\"')\n",
        "\n",
        "    # Preprocess the uploaded image\n",
        "    preprocessed_input_image = preprocess_single_image_from_bytes(file_bytes, TARGET_SIZE)\n",
        "\n",
        "    if preprocessed_input_image is not None:\n",
        "        # Make a prediction\n",
        "        predictions = loaded_model.predict(preprocessed_input_image)\n",
        "\n",
        "        # Get the predicted class\n",
        "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "        predicted_blood_group = reverse_label_map[predicted_class_index]\n",
        "        confidence = np.max(predictions) * 100\n",
        "\n",
        "        print(f\"\\nPredicted Blood Group for {filename}: {predicted_blood_group}\")\n",
        "        print(f\"Confidence: {confidence:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Image preprocessing failed for {filename}.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3e2b006-22db-4d36-8daf-f71a7abf442c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3e2b006-22db-4d36-8daf-f71a7abf442c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 22 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fe023f52ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cluster_6_53.BMP to cluster_6_53.BMP\n",
            "User uploaded file \"cluster_6_53.BMP\"\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\n",
            "Predicted Blood Group for cluster_6_53.BMP: O+\n",
            "Confidence: 99.97%\n"
          ]
        }
      ]
    }
  ]
}